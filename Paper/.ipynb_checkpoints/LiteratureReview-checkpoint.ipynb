{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相關網站資料\n",
    "\n",
    "* [魔高一丈！臉書舉辦Deepfake造假影片辨真大賽找解方，全球2千AI專家參賽最高仍只認出8成](https://www.ithome.com.tw/news/138233)\n",
    "* [FB換臉偵測競賽結果出爐！辨識造假影片第一名準確率為65.18%](https://newtalk.tw/news/view/2020-06-15/421753)\n",
    "* [\n",
    "與假影片誓不兩立！Google AI 開源 Deepfake 檢測數據集，3 千多位真人親自上陣](http://technews.tw/2019/10/01/contributing-data-to-deepfake-detection-research/)\n",
    "   \n",
    "   Face Forensics++ 是由 1,000 支原始影片序列組成的檢測資料庫，這些影片序列透過 4 種自動臉部操作方法操作，即 Deepfakes、face 2 face、faceswap 和 Neural Textures。這些數據源於 977 支 YouTube 影片，所有影片都包含一張沒有遮擋的正面人像，這使自動篡改法能使偽造更真實。\n",
    "\n",
    "   由於此方法提供二進制遮罩，因此這資料庫可用於圖像和影片分類及分割。此外，Face Forensics++ 還提供 1,000 個 Deepfakes 模型生成和擴充新數據。有關更多資訊可參考[網站](https://arxiv.org/abs/1901.08971)\n",
    "   - Deepfake 檢測數據集──加入 Google 與 Jigsaw 發表的 Deepfake [檢測數據集](https://ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html)\n",
    "   - Neural Textures──加入使用 GANs 和 Neural Textures [進行臉部作業的方法](https://arxiv.org/pdf/1904.12356.pdf)。\n",
    "* [FaceForensics++: Learning to Detect Manipulated Facial Images](https://github.com/ondyari/FaceForensics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一篇: [FaceForensics++: Learning to Detect Manipulated Facial Images, 2019/08](https://arxiv.org/pdf/1901.08971.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**DeepFakes**: has shown how computer graphics and visualization techniques can be used to defame persons by replacing their face by the face of a different person\n",
    "\n",
    "##### 兩種主要的DeepFake方法: facial expression(臉部表情) manipulation and facial identity(臉部身份) manipulation\n",
    "\n",
    "![Title](./image/facialReenactment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset 介紹**\n",
    "* Face2Face: 臉部表情操作方法，從一個人的表情置換成另外一個人。如Synthesizing Obama，可以在表情置換後，根據音訊進行臉部動畫處理\n",
    "* FaceSwap: 臉部身份操作方法，整張臉由一個人置換成另外一個人。\n",
    "* 數據集: 包括兩種分類Face2Face及FaceSwap，以兩種learning base方法生成DeepFakes及NeuralTextures。(we generate a large-scale dataset of manipulations based on the classical computer graphics-based methods Face2Face and FaceSwap as well as the learning- based approaches DeepFakes and NeuralTextures.)\n",
    "   - NeuralTextures: 利用原始影片學習目標人的神經紋理 (It uses the original video data to learn a neural texture of the target person, including a rendering network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Forgery Detection**\n",
    "\n",
    "* 結果如下表\n",
    "<div>\n",
    "<img src=\"./image/XceptionNet.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "   1) <font color=green size=3> Cozzolino et al. [17] </font> cast the hand-crafted Steganalysis features from the previous section to a CNN-based network. We finetune this network on our large scale dataset.\n",
    "   \n",
    "   2) We use our dataset to train the convolutional neural network proposed by <font color=green size=3> Bayar and Stamm [10] </font> that uses a constrained convolutional layer followed by <font color=brown size=3> two convolutional, two max-pooling and three fully-connected layers. </font> The constrained convolutional layer is specifically designed to suppress the high-level content of the image. Similar to the previous methods, we use a centered 128 × 128 crop as input.\n",
    "   \n",
    "   3) <font color=green size=3> Rahmouni et al. [51] </font> adopt <font color=brown size=3> different CNN architectures with a global pooling layer that computes four statistics (mean, variance, maximum and minimum). </font> We consider the Stats-2L network that had the best performance.\n",
    "   \n",
    "   4) <font color=green size=3> MesoInception-4 [5] </font> is a CNN-based network inspired by <font color=brown size=3> InceptionNet [56] to detect face tampering in videos. The network has two inception modules and two classic convolution layers interlaced with max-pooling layers. Afterwards, there are two fully-connected layers. Instead of the classic cross-entropy loss, the authors propose the mean squared error between true and predicted labels. </font> We resize the face images to 256 × 256, the input of the network.\n",
    "   \n",
    "   5) <font color=green size=3> XceptionNet [14] </font>  is a traditional CNN trained on ImageNet based on separable convolutions with residual connections. <font color=brown size=3> We transfer it to our task by replacing the final fully connected layer with two outputs. The other layers are initialized with the ImageNet weights. To set up the newly inserted fully connected layer, we fix all weights up to the fi- nal layers and pre-train the network for 3 epochs.  </font>After this step, we train the network for 15 more epochs and choose the best performing model based on validation accuracy.\n",
    "   \n",
    "<div>\n",
    "<img src=\"./image/XceptionNetFinal.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/pdf/1610.02357.pdf)\n",
    "\n",
    "   內有完整的網路架構說明，Xception源自於GoogLenet Inception V3\n",
    "<div>\n",
    "<img src=\"./image/XceptionNetWork.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github中集合眾多文章 [Awesome-Deepfakes-Materials](https://github.com/datamllab/awesome-deepfakes-materials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Deep Learning for Deepfakes Creation and Detection](https://arxiv.org/pdf/1909.11573.pdf)\n",
    "* [DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection](https://arxiv.org/pdf/2001.00179.pdf)\n",
    "* [Media Forensics and DeepFakes: an overview](https://arxiv.org/pdf/2001.06564.pdf)\n",
    "* [Will Deepfakes Do Deep Damage?](https://dl.acm.org/doi/pdf/10.1145/3371409?download=true)\n",
    "\n",
    "#### 以下分別針對此四篇paper進行簡要描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **第二篇** [Deep Learning for Deepfakes Creation and Detection, 2019/09](https://arxiv.org/pdf/1909.11573.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本篇的架構如下圖，在偵測時區分:\n",
    "\n",
    "* **Image**\n",
    "\n",
    "* **Video**\n",
    "\n",
    "   * **Visual Artifacts within Video Frame**: Video在Frame內的加工，再區分深度分類及淺層分類兩種不同方法\n",
    "   \n",
    "   * **Temporal Features across Video Frames**: Frame間有時序的Video是否有Fake的判別"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./image/dldc.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Temporal Features across Video Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./image/2-2.png\" width=\"800\"/>\n",
    "<img src=\"./image/2-3.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visual Artifacts within Video Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. Deep classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Because of the resolution inconsistency between the warped face area and the surrounding context, this process leaves artifacts that can be detected by CNN models such as VGG16 [80], ResNet50, ResNet101 and ResNet152 [81]. A deep learning method to detect deepfakes based on the artifacts observed during the face warping step of the deepfake generation algorithms was proposed in [82]. \n",
    "* 利用VGG-19先提取特徵，給Capsule Network(膠囊網路)，最後以Dynamic Routing(動態路徑)區分真假影像\n",
    "* 查了才發現Capsul Network也是 Hinton大師發明的[Capsule Networks(CapsNets)帶給了我們什麼? 從Geoffrey Hinton談起](https://dosudodl.wordpress.com/2017/12/01/capsule-networkscapsnets帶給了我們什麼-從geoffrey-hinton談起-1-2/?blogsub=confirming#subscribe-blog)\n",
    "* [「膠囊網路」早已是公認將成為下一代深度學習基石CNN「接班人」的神經網路](https://www.techbang.com/posts/70450-capsule-network-is-the-successor-to-the-next-generation-of-deep-learning-artificial-intelligence-neural-network)\n",
    "* [Running CapsuleNet on TensorFlow](https://medium.com/botsupply/running-capsulenet-on-tensorflow-1099f5c67189)\n",
    "* [三巨头共聚AAAI：Capsule没有错，LeCun看好自监督，Bengio谈注意力](https://www.jiqizhixin.com/articles/2020-02-11-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./image/2-4.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2 Shallow classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, a method to exploit artifacts of deepfakes and face manipulations based on visual features of eyes, teeth and facial contours was studied in [93]. The visual artifacts arise from lacking global consistency, wrong or imprecise estimation of the incident illumination, or imprecise estimation of the underlying geometry. For deepfakes detection, missing reflections and missing details in the eye and teeth areas are exploited as well as texture features extracted from the facial region based on facial landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 彙總各種方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./image/2-5.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **第三篇** [DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection, 2020/06](https://arxiv.org/pdf/2001.00179.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 四種類型的面部操縱：i）全臉合成，ii）身份交換（DeepFakes），iii）屬性操縱，以及iv）表情交換。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./image/3-1.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./image/3-2.png\" width=\"800\"/>\n",
    "<img src=\"./image/3-3.png\" width=\"900\"/>\n",
    "<img src=\"./image/3-4.png\" width=\"900\"/>\n",
    "<img src=\"./image/3-5.png\" width=\"900\"/>\n",
    "<img src=\"./image/3-6.png\" width=\"900\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **第四篇** [Media Forensics and DeepFakes: an overview, 2020/01](https://arxiv.org/pdf/2001.06564.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在[158]中，提出將面部不對稱性作為區分特徵，以區分從人臉真實圖像生成的計算機。\n",
    "2. 然後，在[159]中，焦點轉移到視頻上，並且檢測依賴於適合面部的3D模型的時空變形。特別是，自然面要比合成面更複雜，更複雜的幾何變形，並引起3D模型的更高擾動。而且，自然面孔屬於活人。\n",
    "3. 因此，在[160]中提出的方法依賴於由於心跳引起的周期性血流而導致的臉部外觀的微小變化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切換到用於視頻中的深度檢測的解決方案，在[177]中，提出了兩種簡單的體系結構，它們具有少量利用介觀特徵的級別和參數。 \n",
    ">第一個解決方案:（Meso-4）具有四層卷積和池化，然後是具有一個隱藏層的密集網絡。\n",
    "\n",
    ">第二種解決方案:（MesoInception-4）則基於包含擴展卷積的inception模塊的變體。 但是，在[178]中進行的實驗清楚地表明，在有監督的情況下，非常深的通用網絡[179]，[180]，[181]優於以法醫為導向的淺層CNN以及基於手工製作的方法 功能，尤其是在視頻編解碼器具有典型的強壓縮功能的情況下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **第五篇** [Exposing DeepFake Videos By Detecting Face Warping Artifacts](https://arxiv.org/pdf/1811.00656.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結論: \n",
    "\n",
    "    隨著DeepFake背後的技術不斷發展，我們將繼續改進檢測方法。 首先，我們想評估和提高我們的檢測方法在多視頻壓縮方面的魯棒性。\n",
    "    As the technology behind DeepFake keeps evolving, we will continuing improve the detection method. First, we would like to evaluate and improve the robustness of our detection method with regards to multiple video compression. \n",
    "\n",
    "    其次，我們目前正在為此任務使用預先設計的網絡結構（例如resnet或VGG），但是為了更有效地進行檢測，我們想探索專用的網絡結構來檢測DeepFake視頻。\n",
    "    Second, we currently using predesigned network structure for this task (e.g., resnet or VGG), but for more efficient detection, we would like to explore dedicated network struc- ture for the detection of DeepFake videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **第六篇** [Capsule-Forensics: Using Capsule Networks to Detect Forged Images and Videos, 2018/10](https://arxiv.org/pdf/1810.11215.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper presents a method that uses a capsule net- work to detect forged images and videos in a wide range of forgery scenarios(本文提出了一種使用膠囊網絡在多種偽造場景下檢測偽造圖像和視頻的方法), including \n",
    "\n",
    ">replay attack detection and (both fully and partially) computer-generated image/video detection. This is pioneering work in the use of capsule net- works [14, 15, 16], which were originally designed for computer vision problems, to solve digital forensics problems. (重播攻擊檢測以及（全部或部分）計算機生成的圖像/視頻檢測。 這是使用膠囊網絡[14、15、16]的開創性工作，該網絡最初是為解決電腦視覺問題而設計的，旨在解決數位採證問題。)\n",
    "   > * [replay attack wiki說明](https://zh.wikipedia.org/wiki/重放攻击)\n",
    "   \n",
    ">A comprehensive survey of state-of-the-art related work and intensive comparisons using four major datasets demonstrated the superior performance of the proposed method.(使用四個主要數據集對最新技術的相關工作進行了全面調查，並進行了深入的比較，結果表明了該方法的優越性能。)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ReplayAttackDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 在深度學習時代之前，LBP方法是抵禦重放攻擊的主要防御手段[7，8]。 金等人介紹的方法。 文獻[17]基於擴散速度的局部模式（局部速度模式），其精度高於基於LBP的方法。\n",
    "2. 現在，隨著深度學習的引入，檢測重播攻擊的能力已大大提高。 楊等人的方法。 [18]使用支持向量機對由預訓練卷積神經網絡（CNN）提取的特徵進行分類。\n",
    "3. Menotti等。 [19]使用類似的過程，但是在可用的高性能CNN架構中優化了過濾器。 Alotaibi和Mahmood [20]的方法在其自己的CNN中使用基於加法算子拆分方案的非線性擴散。 伊藤等人最近介紹的方法。 [21]利用預訓練的CNN，並利用整個圖像而不是僅提取面部區域。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Computer-GeneratedImage/VideoDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有多種用於檢測計算機生成的圖像或視頻的先進方法，例如，\n",
    "\n",
    ">1.一種用於面部交換的Deepfake技術[6]，\n",
    "\n",
    ">2.Face2Face方法用於面部重現[1]\n",
    "\n",
    ">3.以偽造為目的的深層視頻肖像技術[2]。\n",
    "\n",
    "   >* Fridrich和Kodovsky [10]提出了一種基於特徵的手工特徵隱寫分析方法，該方法也可用於偽造檢測。\n",
    "   >* Cozzolino等。 [22]實現了這種方法的CNN版本。 * Raghavendra等。 [23]描述了微調兩個可用CNN的特殊情況\n",
    "   >* 而Rossler等人。 [11]僅使用了一個CNN。\n",
    "   >* Bayar和Stamm [24]，Rahmouni等。 [25]，Afchar等。 [13]，Quan等。 [26]，和李等。 [9]提出了自己的網絡。\n",
    "   >* Li等人的網絡[9]例如基於視頻，並使用時間信息來檢測眨眼。\n",
    "   >* 我們使用了一種混合方法[12]並結合了預先訓練的VGG（視覺幾何組）-19網絡[27]和擬議的CNN的一部分。\n",
    "   >* 周等。 [28]提出了一種兩流網絡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CapsuleNetworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinton等。 [14]解決了將CNN用於逆向圖形任務的局限性，並為2011年更強大的“膠囊”架構奠定了基礎。\n",
    "\n",
    "但是，由於缺乏有效的算法和計算機硬件的限制，這種複雜的體系結構在當時無法有效實施。取而代之的是，易於設計，易於訓練的CNN廣泛使用。\n",
    "\n",
    "現在，隨著動態路由算法[15]和期望最大化路由算法[16]的引入，膠囊網絡已經實現，並取得了令人矚目的初步成果。\n",
    "\n",
    "最近的兩項研究表明，通過動態路由算法計算的膠囊之間的一致性，可以很好地描述對象部分之間的分層姿勢關係。\n",
    "\n",
    "這提高了視覺任務的準確性。膠囊網絡在取證任務中的應用是本文的重點，這是一個具有挑戰性的問題。但是，使用動態路由算法實現的膠囊之間的一致性可以提高對複雜且幾乎無瑕的偽造圖像和視頻的檢測性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CAPSULE-FORENSICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./image/6-1.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinton等。 [14]解決了將CNN用於逆向圖形任務的局限性，並為2011年更強大的“膠囊”架構奠定了基礎。\n",
    "\n",
    "所提出的方法（圖1）適用於圖像和視頻。對於視頻輸入，在預處理階段將視頻分為幾幀。然後從幀中獲取分類結果（後驗概率）。在後處理階段對概率求平均值，以獲得最終結果。其餘部分與輸入圖像相同。\n",
    "\n",
    "在預處理階段，檢測到人臉並將其縮放為128×128。就像我們在以前的工作中所做的[12]一樣，我們使用VGG-19網絡的一部分[27]提取潛在特徵，將其作為輸入到膠囊網絡。與我們之前的工作不同，我們採用第三個 maxpooling 的輸出，而不是ReLU層之前的三個輸出。(we take the output of the third maxpooling layer instead of three outputs before the ReLU layers. )我們這樣做是因為我們需要減小膠囊網絡輸入的大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capsule Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./image/6-2.png\" width=\"500\"/>\n",
    "<img src=\"./image/6-3.png\" width=\"500\"/>\n",
    "<img src=\"./image/6-4.png\" width=\"500\"/>\n",
    "<img src=\"./image/6-5.png\" width=\"500\"/>\n",
    "<img src=\"./image/6-6.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "擬議的網絡由三個主要膠囊和兩個輸出膠囊組成，一個用於真實圖像，一個用於虛假圖像（Fig.2）。 VGG-19網絡的一部分[27]提取的潛在特徵是輸入，它們被分配到三個主要膠囊（Fig.3）。\n",
    "\n",
    "如我們先前的工作[12]中所使用的，統計池對於偽造檢測很重要。\n",
    "\n",
    "使用算法1將三個膠囊（uj | i）的輸出動態路由到輸出膠囊（vj）進行r次迭代。\n",
    "\n",
    "該網絡具有大約280萬個參數，對於此類網絡而言，數量相對較少。 我們稍微改進了Sabour等人的算法。 [15]通過將高斯隨機噪聲添加到3-D權重張量W並在進行迭代路由之前應用一個額外的squash（等式1）。\n",
    "\n",
    "增加的噪聲有助於減少過度擬合，同時附加方程式可保持網絡更穩定。 主膠囊和輸出膠囊的輸出如Fig.4所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第七篇 [USE OF A CAPSULE NETWORK TO DETECT FAKE IMAGES AND VIDEOS, 2019/10](https://arxiv.org/pdf/1910.12467.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "與上篇是同一組人的第二版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./image/7-1.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[github](https://github.com/nii-yamagishilab/Capsule-Forensics-v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a variation of the XceptionNet [76] as a base- line classifier and trained it in accordance with the guidelines used by Rossler et al. [27]. In this research, it achieved state-of-the-art performance on detecting manipulated im- ages. However, the XceptionNet is a large network with more than 20 million parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forensics with ten primary capsules) with the previous ver- sion (Capsule-Forensics (old)) [28], as listed in Table 2. \n",
    ">The previous version included three primary capsules and had the option of adding random noise during the training process to reduce overfitting. \n",
    "\n",
    ">For the new versions, one of the new set- tings enabled the use of a larger input size (300 × 300 instead of 128 × 128), meaning that a larger number of important artifacts in the images. \n",
    "\n",
    "Another new setting enabled the use of more capsules (ten capsules instead of three), which increased the network capability. \n",
    ">We call the 3-capsule version the light one. \n",
    "\n",
    ">The last new setting enabled the use of dropout during the training process, which also played the role of a regularizer. \n",
    "\n",
    ">This final version is called Capsule-Forensics + Dropout + Noise (300 × 300) in Table 2. \n",
    "\n",
    ">For simplicity, we also evaluated the final version of the Capsule-Forensics on video inputs by applying frame aggregation by calculating the average of the classification probabilities on the first ten frames (named as Capsule-Forensics + Dropout + Noise (video). \n",
    "\n",
    "All versions of Capsule-Forensics were trained with 25 epochs. The results for XceptionNet and all versions of Capsule-Forensics on the test set are shown in Table 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
